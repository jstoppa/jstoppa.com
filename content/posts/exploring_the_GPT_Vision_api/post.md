---
author: Juan Stoppa
title: Exploring the GPT-4 with Vision API
summary: This article explores the capabilities of the GPT-4 Vision API with an example of each case including more complex scenarios such as object location.
date: 2024-01-08
description: All you need to know to understand the GPT-4 with Vision API.
draft: false
math: true
tags: ["openai", "python", "chatgpt", "gpt-4", "gpt4-vision"]
cover:
    image: "posts/exploring_the_GPT_Vision_api/GPT-4 Vision2.webp"
    caption: GPT-4 with Vision generated by ChatGPT of course!
twitter:
    card: summary_large_image
    site: "@juanstoppa"
    title: Exploring the GPT-4 with Vision API
    description: All you need to know to understand the GPT-4 with Vision API.
---

I've been exploring the GPT-Vision API and I've been blown away at what is capable of, as Open AI puts it, ChatGPT can now see, hear and speak. But how good is the API? I'm doing a deep dive in this article with all the technical details and results.

## What is GPT-4 with Vision API to start with?

GPT-4 Turbo with Vision is an advanced large multimodal model (LMM) created by OpenAI, capable of interpreting images and offering textual answers to queries related to these images. This model blends the capabilities of visual perception with the natural language processing. 

## The Basic - How to use it?

The API is used in a similar way as the normal completion API and I recommend using Python to interact with it as it's the best language well supported by the Open AI documentation. If you are new and using Windows, you can see my article [Getting started with OpenAI in Python](http://localhost:1313/posts/getting_started_with_openai_in_python/post/) where are explained all the details on how to setup Python and interact with Open AI


