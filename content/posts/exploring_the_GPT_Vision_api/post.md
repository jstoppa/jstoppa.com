---
author: Juan Stoppa
title: Exploring the GPT-4 with Vision API
summary: This article explores the capabilities of the GPT-4 Vision API with an example of each case including more complex scenarios such as object location.
date: 2024-01-08
description: All you need to know to understand the GPT-4 with Vision API.
draft: false
math: true
tags: ["openai", "python", "chatgpt", "gpt-4", "gpt4-vision"]
cover:
    image: "posts/exploring_the_GPT_Vision_api/GPT-4 Vision2.webp"
    caption: GPT-4 with Vision generated by ChatGPT of course!
twitter:
    card: summary_large_image
    site: "@juanstoppa"
    title: Exploring the GPT-4 with Vision API
    description: All you need to know to understand the GPT-4 with Vision API.
---

I've been exploring the GPT-Vision API and I have been blown away by what is capable of. As Open AI describes it, ChatGPT can now see, hear, and speak. But how effective is the API? In this article, I'm doing a deep dive into the GPT vision API describing all the technical details. 

## What is GPT-4 with Vision API to start with?

GPT-4 Turbo with Vision is an advanced large multimodal model (LMM) created by OpenAI, capable of interpreting images and offering textual answers to queries related to these images. This model blends the capabilities of visual perception with the natural language processing. 

## The Basic - How to use it?

The API is used in a way similar to the standard completion API, and I recommend using Python for integration with the API as it's the best-supported language in Open AI's documentation. If you are new to this and using Windows, you might find my article [Getting started with OpenAI in Python](http://localhost:1313/posts/getting_started_with_openai_in_python/post/) helpful where I explain all the details on how to set up Python and interact with OpenAI.






